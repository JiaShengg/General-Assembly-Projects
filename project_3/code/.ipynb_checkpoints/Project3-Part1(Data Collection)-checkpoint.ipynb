{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712203c7",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP (Part 1 Data Collection)\n",
    "#### Binary CLassification of Subreddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9813d",
   "metadata": {},
   "source": [
    " - [Problem Statement](#Problem-Statement)\n",
    " - [Executive Summary](#Executive-Summary)\n",
    " - [Methodology](#Methodology)\n",
    " - [Datasets](#Datasets)\n",
    " - [Data Import & Setup Conditions](#Data-Import-&-Setup-Conditions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c04a5",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0c835",
   "metadata": {},
   "source": [
    "We've been tasked by a matchmaker company (Swindler Inc) to construct a binary classification model that can distinguish genuine relationship advice questions (subreddit r/relationship advise) from potential spam and non-related enquiries (subreddit r/stupidquestions). Seven classification models will be created for this project: Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, Support Vector Machines and Gradient Boosting with accuracy, precision and F1 score on unseen test data used to determine success. Swindler Inc. relationship experts will be key stakeholders in the endeavor. The intention for this study is for relationship specialists to spend as little time as possible reviewing emails in order to determine which are spam. The time saved can then be put to better use, such as increasing production or providing clients with more specialized relationship guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fa222",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f54006",
   "metadata": {},
   "source": [
    "Working professionals who don't have enough time to meet people in person owing to their tight work schedule are increasingly using Swindler dating app. It not only makes dating easy and comfortable for working professionals, but it also provides customized relationship advice. However, Swindler dating app receives a flood of inquiries for advises on a daily basis, and they fear that some of these inquiries are the result of foul play by competitors.\n",
    "\n",
    "As such, they've enlisted the expertise of well-known NLP experts to develop a binary classification model that can tell the difference between genuine relationship guidance enquiries and non-related inquiries. NLP experts advocate collecting data from Reddit, a social news network that allows users to discuss and vote on articles published by other users, in order to train a suitable model. The model will be developed using the following two Reddit threads: genuine relationship inquiries from r/relationship advise and unrelated questions from r/stupidquestion.\n",
    "\n",
    "Seven classification models were developed for this binary classification project: Naive Bayes, Logistic Regression, Random Forest, Decision Tree, K-Nearest Neighbors, Support Vector Machines and Gradient Boosting with accuracy, precision and F1 score on unseen test data used to determine success.\n",
    "\n",
    "When it comes to training and testing gridsearch metric scores, both the Logistic and GradientBoostingClassifier models perform equally well after hyperparameter adjustment. However, because it is the simplest/easiest model for the audience to understand, we used the logistic regression model with countvectorizer as our final model in this study. On our testing dataset, it has the greatest accuracy of 0.96161, precision of 0.9555, and F1 score of 0.9621. However, there is still some overfitting between the training and test datasets.\n",
    "\n",
    "As such, moving forward, some recommendations to further improve the model as follows:\n",
    "- Use the latest classification modeling technique such as catboost to check for improvement of performance\n",
    "- Conduct lemmatization for both topics \n",
    "- Perform Sentiment analysis of the 2 topics and observed for obvious differences\n",
    "- Expand STOPWORDS library to include most frequently misclassified words\n",
    "- Perform gridsearch for all 7 classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29280e8",
   "metadata": {},
   "source": [
    "## Methodology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd615be1",
   "metadata": {},
   "source": [
    "Following Blitzstein & Pfisterâ€™s workflow ([*source3*](https://github.com/cs109/2015/blob/master/Lectures/01-Introduction.pdf)), a 5 steps framework was implemented to conduct this analysis. These 5 steps are:\n",
    "\n",
    "**Step 1: Ask an interesting question**\n",
    "- Defining a clear and concise problem statement.\n",
    "\n",
    "**Step 2: Get the data**\n",
    "- Import and clean raw data to ensure that all datatypes were accurate and any other errors were fixed.\n",
    "\n",
    "**Step 3: Explore the data**\n",
    "- Check for duplicated posts for each topic\n",
    "- Plot visualization for distribution of posts of each topic\n",
    "- Feature engineering\n",
    "- Remove URLS / punctuations / NON-ASCII / Stopwords for each posts\n",
    "\n",
    "**Step 4: Model the data**\n",
    "- Creating a base model with MultinomialNB model\n",
    "- Compare success metrics between the different classification models after hyperparameters tuning\n",
    "- Selecting the best Machine learning algorithm/model selection for submission\n",
    "- Data Visualization\n",
    "  - barplots\n",
    "  - histograms\n",
    "  - SHAP summary plot\n",
    "\n",
    "**Step 5: Communicate and visualize the results**\n",
    "- Present findings to a non-technical audience and provide recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ef82a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e1cfc",
   "metadata": {},
   "source": [
    "* [`genuine_question.csv`](../datasets/genuine_qns.csv): Data set contains genuine relationship questions. This dataset will be split for  training and testing purposes.\n",
    "* [`unrelated_question.csv`](../datasets/unrelated_qns.csv): Data set contains unrelated questions. This dataset will be split for  training and testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c119908",
   "metadata": {},
   "source": [
    "## Data Import & Setup Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65655bdb",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fc6ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime as dt\n",
    "from pmaw import PushshiftAPI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa7c1a",
   "metadata": {},
   "source": [
    "#### Using Pushshift API to extract 8000 posts from subreddit r/relationship_advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a25d6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8000 submissions from Pushshift\n"
     ]
    }
   ],
   "source": [
    "api = PushshiftAPI()\n",
    "\n",
    "subreddit = 'relationship_advice'\n",
    "limit = 8000\n",
    "before = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2010,1,1,0,0).timestamp())\n",
    "rate_limit = 100 #To prevent overloading the server with requests, we set parameters such as rate limit and max sleep of 10 sec after every 100 posts\n",
    "max_sleep = 10 \n",
    "\n",
    "real_qns_data = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after, rate_limit=rate_limit, max_sleep=max_sleep)\n",
    "print(f'Retrieved {len(real_qns_data)} submissions from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2275218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the data into a dataframe\n",
    "real_qns_df = pd.DataFrame(real_qns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "546a423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the columns to contain only the subreddit name, title and content of the post\n",
    "real_qns_df = real_qns_df[['subreddit', 'selftext', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1db6bc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_qns_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf9ed5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit     0\n",
       "selftext     11\n",
       "title         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some of the post's content is devoid of words and instead consists of images, GIFs, or videos. However, since there is only 11/8000 posts without content, this is acceptable\n",
    "real_qns_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9922449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#savings the dataframe to csv format\n",
    "real_qns_df.to_csv('../datasets/genuine_qns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ce9ee",
   "metadata": {},
   "source": [
    "#### Using Pushshift API to extract 8000 posts from subreddit r/stupidquestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3244f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n",
      "Not all PushShift shards are active. Query results may be incomplete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 8000 submissions from Pushshift\n"
     ]
    }
   ],
   "source": [
    "subreddit = 'stupidquestions'\n",
    "limit = 8000\n",
    "before = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2010,1,1,0,0).timestamp())\n",
    "#rate_limit = 60 #To prevent overloading the server with requests, we set parameters such as rate limit and max sleep of 10 sec after every 100 posts\n",
    "#max_sleep = 90\n",
    "\n",
    "unrelated_qns_data = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after) #, rate_limit=rate_limit, max_sleep=max_sleep)\n",
    "print(f'Retrieved {len(unrelated_qns_data)} submissions from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8130f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the data into a dataframe\n",
    "unrelated_qns_df = pd.DataFrame(unrelated_qns_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5b966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering the columns to contain only the subreddit name, title and content of the post\n",
    "unrelated_qns_df = unrelated_qns_df[['subreddit', 'selftext', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48c41a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrelated_qns_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fa2818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit    0\n",
       "selftext     6\n",
       "title        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some of the post's content is devoid of words and instead consists of images, GIFs, or videos. However, since there is only 6/8000 posts without content, this is acceptable\n",
    "unrelated_qns_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cddc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#savings the dataframe to csv format\n",
    "unrelated_qns_df.to_csv('../datasets/unrelated_qns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686458af",
   "metadata": {},
   "source": [
    "#### Continue in Project3-Part2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
