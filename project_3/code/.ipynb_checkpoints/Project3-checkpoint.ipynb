{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "712203c7",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP \n",
    "#### Binary CLassification of Subreddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce9813d",
   "metadata": {},
   "source": [
    " - [Problem Statement](#Problem-Statement)\n",
    " - [Background Information](#Background-Information)\n",
    " - [Methodology](#Methodology)\n",
    " - [Datasets](#Datasets)\n",
    " - [Data Import & Cleaning](#Data-Import-&-Cleaning)\n",
    " - [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    " - [Modeling & Feature Engineering](#Modeling-&-Feature-Engineering)\n",
    " - [Significant Findings](#Significant-Findings)\n",
    " - [Conclusion](#Conclusion)\n",
    " - [Recommendations](#Recommendations)\n",
    " - [Citations and Sources](#Citations-and-Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c04a5",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b0c835",
   "metadata": {},
   "source": [
    "As data scientists engaged by Propnex, we are tasked to develop an accurate model to predict house sales in Ames, Lowas based on the given fixed house characteristics. We will approach this analysis by first modeling the sales prices of houses with default features from the dataset, and later performing features selection to optimize the model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a33cd",
   "metadata": {},
   "source": [
    "## Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f54006",
   "metadata": {},
   "source": [
    "Many real estate organizations have traditionally relied on a mix of intuition and traditional, retrospective statistics to make pricing decisions ([*source1*](https://www.mckinsey.com/industries/real-estate/our-insights/getting-ahead-of-the-market-how-big-data-is-transforming-real-estate)). Today, a slew of new variables, such as proximity to points of interest and the existence of environmental pollution, allow for more vivid depictions of a location's future risks and benefits ([*source2*](https://www.redirectconsulting.com/blog/big-data-in-real-estate-3-important-non-traditional-data-sets-to-consider)).\n",
    "\n",
    "The sweet spot between density and proximity to community amenities differs by American city and even neighborhood, hidden by an ever-increasing amount of data that is becoming increasingly difficult to manage. For instance, while the impact of the proximity of places of interest on property price is obvious, housing values are also influenced by the number, mix, and quality of community amenities that surround them ([*source1*](https://www.mckinsey.com/industries/real-estate/our-insights/getting-ahead-of-the-market-how-big-data-is-transforming-real-estate)).\n",
    "\n",
    "These nonlinear interactions can be found in a variety of American cities. Thus, machine learning technique is one way to stitch together these data through sophisticated analytics, making it substantially easier to comprehend complex relationships.\n",
    "\n",
    "A successful data-driven approach can yield powerful insights. In this analysis, we will attempt to develop an accurate and reliable machine-learning model to forecast the housing price in Ames, Lowas based on the given fixed house characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29280e8",
   "metadata": {},
   "source": [
    "## Methodology "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd615be1",
   "metadata": {},
   "source": [
    "Following Blitzstein & Pfisterâ€™s workflow ([*source3*](https://github.com/cs109/2015/blob/master/Lectures/01-Introduction.pdf)), a 5 steps framework was implemented to conduct this analysis. These 5 steps are:\n",
    "\n",
    "**Step 1: Ask an interesting question**\n",
    "- Defining a clear and concise problem statement.\n",
    "\n",
    "**Step 2: Get the data**\n",
    "- Import and clean raw data to ensure that all datatypes were accurate and any other errors were fixed.\n",
    "- Exploring best method to fill up null values, if applicable\n",
    "\n",
    "**Step 3: Explore the data**\n",
    "- Differentiate numerical and categorical features in the dataset\n",
    "- For categorical features, analyze if they are nominal or ordinal features\n",
    "- Transform ordinal features to numerical ranks\n",
    "- Perform exploratory data analysis to determine any meaningful correlations\n",
    "- Dealing with outliers\n",
    "- Perform feature engineering\n",
    "\n",
    "**Step 4: Model the data**\n",
    "- Creating a base model with Linear Regression\n",
    "- Perform feature selections/ feature engineering to optimize model performance\n",
    "- Selecting the best Machine learning algorithm/model selection for submission\n",
    "- Data Visualization\n",
    "  - subplots\n",
    "  - histograms\n",
    "  - scatterplots\n",
    "  - boxplots\n",
    "\n",
    "**Step 5: Communicate and visualize the results**\n",
    "- Present findings to a non-technical audience and provide recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329ef82a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e1cfc",
   "metadata": {},
   "source": [
    "* [`real_advise.csv`](../datasets/real_advise.csv): Data set contains real relationship advises. This dataset will be split for  training and testing purposes.\n",
    "* [`useless_advise.csv`](../datasets/useless_advise.csv): Data set contains useless advises. This dataset will be split for  training and testing purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c119908",
   "metadata": {},
   "source": [
    "## Data Import from Reddit & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65655bdb",
   "metadata": {},
   "source": [
    "#### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc6ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import requests\n",
    "import datetime as dt\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor, BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aa7c1a",
   "metadata": {},
   "source": [
    "#### Using Pushshift API to extract 8000 posts from subreddit r/relationship_advise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25d6835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 0 submissions from Pushshift\n"
     ]
    }
   ],
   "source": [
    "# api = PushshiftAPI()\n",
    "\n",
    "# subreddit = 'relationship_advice'\n",
    "# limit = 8000\n",
    "# before = int(dt.datetime(2022,1,1,0,0).timestamp())\n",
    "# after = int(dt.datetime(2010,1,1,0,0).timestamp())\n",
    "\n",
    "# real_advise_data = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "# print(f'Retrieved {len(real_advise_data)} submissions from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2275218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_advise_df = pd.DataFrame(real_advise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "546a423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_advise_df = real_advise_df[['subreddit', 'selftext', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1db6bc8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_advise_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf9ed5e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subreddit     0\n",
       "selftext     18\n",
       "title         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_advise_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c9922449",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_advise_df.to_csv('real_advise.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ce9ee",
   "metadata": {},
   "source": [
    "#### Using Pushshift API to extract 8000 posts from subreddit r/LearnUselessTalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e07a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 0 submissions from Pushshift\n"
     ]
    }
   ],
   "source": [
    "api = PushshiftAPI()\n",
    "\n",
    "subreddit = 'relationship_advice'\n",
    "limit = 100\n",
    "before = int(dt.datetime(2021,2,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2020,12,1,0,0).timestamp())\n",
    "\n",
    "\n",
    "useless_advise_data = api.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after)\n",
    "print(f'Retrieved {len(useless_advise_data)} submissions from Pushshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c8130f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_advise_df = pd.DataFrame(useless_advise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "cf89ed47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_advise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4c5b966a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['subreddit', 'selftext', 'title'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-36cbd2e5399a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0museless_advise_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0museless_advise_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subreddit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'selftext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3028\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3030\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3032\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1266\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['subreddit', 'selftext', 'title'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "useless_advise_df = useless_advise_df[['subreddit', 'selftext', 'title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "48c41a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_advise_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9fa2818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useless_advise_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4cddc3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_advise_df.to_csv('useless_advise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686458af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
